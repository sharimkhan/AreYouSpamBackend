{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook Contains the code that analyzes the dataset for most commonly used spam words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loads in the dataset of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>b'People should not have to worry about being shot while shopping or attending school. This is getting ridiculous. Co https://t.co/83LVF1URjk'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b'I Need To Go Shopping! I Need New Clothes '</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>b'@hawillisdc I am still shopping for a good s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Funny, you start charging a $1 deposit for s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Entre tapas &amp;amp; tapas &amp;amp; socos &amp;amp; ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>b'Google Shopping Via Image Optimization:Secre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  \\\n",
       "0  0   \n",
       "1  0   \n",
       "2  0   \n",
       "3  0   \n",
       "4  0   \n",
       "\n",
       "  b'People should not have to worry about being shot while shopping or attending school. This is getting ridiculous. Co https://t.co/83LVF1URjk'  \n",
       "0      b'I Need To Go Shopping! I Need New Clothes '                                                                                              \n",
       "1  b'@hawillisdc I am still shopping for a good s...                                                                                              \n",
       "2  b'Funny, you start charging a $1 deposit for s...                                                                                              \n",
       "3  b'Entre tapas &amp; tapas &amp; socos &amp; ch...                                                                                              \n",
       "4  b'Google Shopping Via Image Optimization:Secre...                                                                                              "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "df = pd.read_table(\"results1.txt\",sep=\",\")\n",
    "df = df.drop(df.columns[df.columns.str.contains('unnamed',case = False)],axis = 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13058\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words method was implemented using the CountVectorizer method from SciKit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer(stop_words='english',min_df = 0.001)\n",
    "df.columns = ['label', 'tweet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sorts every word in tweets labeled spam by frequency. The most frequent words are giveaway(1587), away(1537), click(1428), crypotcurrency(1124), and win(903). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 8333), ('giveaway', 1587), ('away', 1537), ('click', 1428), ('cryptocurrency', 1124), ('win', 903), ('free', 584), ('gt', 525), ('just', 505), ('enter', 498), ('kardashian', 462), ('bitcoin', 448), ('amp', 430), ('crypto', 354), ('kim', 344), ('news', 341), ('new', 339), ('nhttps', 335), ('join', 299), ('watch', 292), ('magazine', 271), ('celebrity', 257), ('people', 248), ('000', 248), ('celeb', 243), ('video', 235), ('celebs', 233), ('100', 232), ('time', 224), ('like', 223), ('live', 217), ('im', 217), ('blockchain', 217), ('check', 216), ('want', 210), ('gift', 206), ('btc', 199), ('youtube', 191), ('kimkardashian', 186), ('giving', 185), ('2018', 179), ('nclick', 176), ('airdrop', 175), ('entered', 173), ('love', 169), ('don', 165), ('day', 163), ('chance', 160), ('10', 157), ('snapchat', 156), ('nadd', 154), ('card', 153), ('dont', 151), ('ethereum', 151), ('big', 150), ('good', 147), ('come', 138), ('buy', 133), ('going', 132), ('great', 126), ('goal', 125), ('support', 122), ('know', 122), ('year', 121), ('game', 120), ('code', 118), ('register', 117), ('khloe', 117), ('money', 116), ('help', 114), ('project', 114), ('online', 111), ('best', 111), ('amazon', 110), ('today', 107), ('need', 106), ('eth', 106), ('pay', 105), ('make', 104), ('got', 104), ('movie', 101), ('right', 101), ('ico', 101), ('playing', 97), ('using', 96), ('bucks', 95), ('thanks', 95), ('easy', 93), ('stuff', 93), ('music', 91), ('way', 90), ('pictures', 90), ('tickets', 89), ('ends', 87), ('follow', 87), ('prizes', 86), ('link', 86), ('extra', 86), ('did', 85), ('50', 83), ('awesome', 83), ('listen', 81), ('really', 79), ('liked', 79), ('supply', 77), ('arsenal', 77), ('fortnite', 76), ('life', 75), ('world', 75), ('spring', 75), ('listia', 75), ('credits', 75), ('let', 74), ('winner', 73), ('doing', 72), ('icymi', 71), ('25', 71), ('ll', 71), ('sign', 69), ('token', 69), ('nfollow', 69), ('man', 68), ('selling', 67), ('pumpkinpayteam', 67), ('ppkc', 67), ('market', 66), ('npumpkin', 66), ('500', 65), ('week', 65), ('nano', 65), ('gfuelenergy', 65), ('home', 64), ('winning', 64), ('details', 64), ('200', 64), ('pyeongchang', 64), ('earning', 64), ('price', 63), ('month', 62), ('worth', 62), ('april', 62), ('daily', 61), ('iota', 61), ('digitalstormpc', 61), ('fazeup', 61), ('15', 60), ('book', 60), ('left', 60), ('learn', 59), ('use', 59), ('ve', 58), ('vlog', 58), ('open', 56), ('cash', 56), ('think', 56), ('prize', 56), ('featured', 56), ('artistrack', 56), ('baby', 56), ('sexy', 55), ('submissions', 55), ('read', 53), ('sweepstakes', 53), ('trading', 53), ('youre', 53), ('litecoin', 53), ('20', 52), ('organized', 52), ('sure', 52), ('contest', 52), ('hope', 52), ('black', 51), ('pc', 51), ('minutes', 51), ('platform', 51), ('guys', 50), ('look', 50), ('actually', 50), ('stream', 50), ('battle', 50), ('followers', 50), ('sale', 49), ('say', 49), ('30', 49), ('1000', 49), ('soon', 49), ('days', 49), ('family', 48), ('trying', 48), ('late', 48), ('coin', 47), ('work', 47), ('play', 47), ('books', 47), ('looking', 47), ('amazing', 47), ('lol', 47), ('coming', 47), ('review', 46), ('gonna', 46), ('shit', 46), ('bank', 45), ('hit', 45), ('shows', 45), ('friends', 45), ('ass', 44), ('start', 44), ('years', 44), ('retweet', 44), ('team', 44), ('ripple', 44), ('pink', 43), ('business', 43), ('tonight', 43), ('twitter', 43), ('season', 43), ('hey', 43), ('running', 43), ('uber', 43), ('tax', 43), ('sex', 42), ('march', 42), ('nthe', 42), ('update', 42), ('happy', 42), ('fantasy', 42), ('save', 42), ('set', 42), ('final', 42), ('long', 42), ('kourtney', 41), ('things', 41), ('wanna', 41), ('try', 41), ('tokens', 41), ('taylor', 41), ('thank', 41), ('community', 41), ('eco', 41), ('massive', 41), ('service', 40), ('jenner', 40), ('ni', 40), ('real', 39), ('maybe', 39), ('having', 38), ('plus', 38), ('blog', 38), ('latest', 38), ('pro', 38), ('twitch', 38), ('edition', 38), ('koa', 38), ('babe', 37), ('12', 37), ('does', 37), ('ticket', 37), ('games', 37), ('hard', 37), ('winners', 37), ('paypal', 37), ('n1', 36), ('miss', 36), ('air', 36), ('party', 36), ('story', 36), ('believe', 36), ('feel', 36), ('half', 36), ('copy', 36), ('swift', 36), ('yes', 36), ('mother', 36), ('amazongiveaway', 36), ('getting', 35), ('available', 35), ('says', 35), ('stay', 35), ('won', 35), ('tell', 35), ('th', 35), ('better', 35), ('congress', 35), ('windows', 35), ('wants', 34), ('tomorrow', 34), ('gets', 34), ('men', 34), ('trump', 34), ('little', 33), ('view', 33), ('old', 33), ('fuck', 33), ('phone', 33), ('dump', 33), ('decentralized', 33), ('usd', 33), ('gaming', 33), ('royale', 33), ('heart', 32), ('end', 32), ('crowdfunding', 32), ('special', 32), ('taking', 32), ('talk', 32), ('future', 32), ('thing', 32), ('tweet', 32), ('ccklml', 32), ('exchange', 32), ('origin', 32), ('pack', 32), ('selfies', 32), ('marketing', 31), ('24', 31), ('choice', 31), ('share', 31), ('8rnluayhzd', 31), ('added', 31), ('super', 31), ('person', 31), ('interested', 31), ('subs', 31), ('investors', 31), ('pump', 31), ('founder', 31), ('approves', 31), ('immediately', 31), ('style', 30), ('pre', 30), ('thats', 30), ('camera', 30), ('huge', 30), ('visit', 30), ('receive', 30), ('wins', 30), ('care', 30), ('list', 30), ('shirt', 30), ('said', 30), ('ball', 30), ('famous', 30), ('mexicos', 30), ('altcoins', 30), ('markets', 30), ('fan', 29), ('11', 29), ('offer', 29), ('makes', 29), ('took', 29), ('pretty', 29), ('40', 29), ('bad', 29), ('bounty', 29), ('calls', 29), ('didnt', 29), ('regulations', 29), ('altcoin', 29), ('shopping', 28), ('job', 28), ('sent', 28), ('sell', 28), ('instagram', 28), ('lt', 28), ('mom', 28), ('release', 28), ('fun', 28), ('west', 28), ('earn', 28), ('stop', 28), ('post', 28), ('white', 27), ('booty', 27), ('giuseppe', 27), ('oh', 27), ('clic', 27), ('99', 27), ('second', 27), ('kylie', 27), ('competition', 27), ('fucking', 27), ('facebook', 27), ('celebrate', 27), ('weekend', 27), ('excited', 27), ('digital', 27), ('currency', 27), ('hour', 27), ('fans', 27), ('ill', 27), ('yasss', 27), ('27', 26), ('echo', 26), ('car', 26), ('action', 26), ('meet', 26), ('thought', 26), ('info', 26), ('trade', 26), ('god', 26), ('night', 26), ('america', 26), ('content', 26), ('used', 26), ('invest', 26), ('mining', 26), ('run', 26), ('faze', 26), ('vitalik', 26), ('bonus', 25), ('forget', 25), ('18', 25), ('dogs', 25), ('hot', 25), ('ready', 25), ('change', 25), ('xbox', 25), ('kids', 25), ('entry', 25), ('war', 25), ('cryptocurrencies', 25), ('250', 25), ('throw', 25), ('caught', 25), ('sunglasses', 25), ('xv', 25), ('originpcfamily', 25), ('vbucks', 25), ('gay', 24), ('thousands', 24), ('referral', 24), ('value', 24), ('00', 24), ('credit', 24), ('place', 24), ('program', 24), ('seen', 24), ('beauty', 24), ('website', 24), ('pick', 24), ('draft', 24), ('80', 24), ('lost', 24), ('dead', 24), ('binance', 24), ('wanted', 24), ('bump', 24), ('pregnant', 24), ('artwork', 24), ('atletico', 24), ('android', 23), ('women', 23), ('gold', 23), ('success', 23), ('dating', 23), ('winter', 23), ('iphone', 23), ('power', 23), ('order', 23), ('law', 23), ('photo', 23), ('played', 23), ('beyonce', 23), ('13', 23), ('walk', 23), ('tokyo', 23), ('closing', 23), ('perfect', 23), ('cool', 23), ('hi', 23), ('hours', 23), ('break', 23), ('didn', 23), ('authority', 23), ('baffled', 23), ('nude', 23), ('summer', 23), ('sbd', 23), ('influencers', 23), ('14', 22), ('fast', 22), ('launch', 22), ('fryer', 22), ('olympics', 22), ('google', 22), ('february', 22), ('app', 22), ('starting', 22), ('page', 22), ('pain', 22), ('signed', 22), ('pair', 22), ('south', 22), ('lucky', 22), ('economy', 22), ('guy', 22), ('3rd', 22), ('goes', 22), ('belgian', 22), ('yellow', 22), ('giveaways', 22), ('lot', 21), ('girls', 21), ('bet', 21), ('private', 21), ('article', 21), ('club', 21), ('biggest', 21), ('multi', 21), ('roku', 21), ('st', 21), ('lights', 21), ('tips', 21), ('shop', 21), ('milf', 21), ('download', 21), ('trip', 21), ('million', 21), ('following', 21), ('yeah', 21), ('photos', 21), ('company', 21), ('case', 21), ('idea', 21), ('bought', 21), ('steemit', 21), ('announce', 21), ('participated', 21), ('infinitywar', 21), ('avengers', 21), ('2nd', 20), ('account', 20), ('minded', 20), ('gaysingles', 20), ('friendships', 20), ('relationships', 20), ('social', 20), ('appreci', 20), ('vs', 20), ('city', 20), ('package', 20), ('event', 20), ('whore', 20), ('ask', 20), ('ad', 20), ('mobile', 20), ('group', 20), ('comes', 20), ('bring', 20), ('worlds', 20), ('girl', 20), ('fight', 20), ('passport', 20), ('steam', 20), ('ps4', 20), ('yo', 20), ('finally', 20), ('true', 20), ('food', 20), ('yall', 20), ('xrp', 20), ('fintech', 20), ('tron', 20), ('sister', 20), ('box', 20), ('phanteks', 20), ('sub', 20), ('email', 19), ('close', 19), ('apple', 19), ('beautiful', 19), ('banks', 19), ('hair', 19), ('doesnt', 19), ('jennifer', 19), ('subscriber', 19), ('profile', 19), ('high', 19), ('members', 19), ('tour', 19), ('purchase', 19), ('song', 19), ('instant', 19), ('young', 19), ('gives', 19), ('hear', 19), ('cards', 19), ('ceremony', 19), ('complete', 19), ('art', 19), ('hand', 19), ('korea', 19), ('strategy', 19), ('income', 19), ('wish', 19), ('subscription', 19), ('makeup', 19), ('wallet', 19), ('remember', 19), ('road', 19), ('keystone', 19), ('rov', 19), ('store', 18), ('wrong', 18), ('gaydating', 18), ('gaypersonals', 18), ('loves', 18), ('tv', 18), ('children', 18), ('sunday', 18), ('called', 18), ('congratulations', 18), ('training', 18), ('king', 18), ('north', 18), ('reason', 18), ('cannabis', 18), ('nif', 18), ('president', 18), ('hurry', 18), ('making', 18), ('national', 18), ('participate', 18), ('official', 18), ('wait', 18), ('kindle', 18), ('300', 18), ('needs', 18), ('basic', 18), ('nit', 18), ('changing', 18), ('secure', 18), ('working', 18), ('coinbase', 18), ('manna', 18), ('nyou', 18), ('announces', 18), ('probably', 18), ('wow', 18), ('outdoor', 18), ('cover', 17), ('led', 17), ('tech', 17), ('times', 17), ('hollywood', 17), ('fact', 17), ('lawrence', 17), ('picnic', 17), ('nice', 17), ('smart', 17), ('episode', 17), ('funny', 17), ('ultimate', 17), ('26', 17), ('global', 17), ('skin', 17), ('paid', 17), ('23', 17), ('vote', 17), ('fake', 17), ('tried', 17), ('bit', 17), ('hands', 17), ('romance', 17), ('diamond', 17), ('reading', 17), ('customers', 17), ('based', 17), ('international', 17), ('trx', 17), ('returns', 17), ('affiliate', 17), ('far', 17), ('japan', 17), ('giftcard', 17), ('pass', 17), ('madrid', 17), ('laptop', 16), ('original', 16), ('butt', 16), ('takes', 16), ('doesn', 16), ('chat', 16), ('nba', 16), ('fund', 16), ('public', 16), ('pornstar', 16), ('pussy', 16), ('friend', 16), ('nwe', 16), ('coins', 16), ('feeling', 16), ('kanye', 16), ('sold', 16), ('couple', 16), ('body', 16), ('finance', 16), ('government', 16), ('living', 16), ('brand', 16), ('kardashians', 16), ('funding', 16), ('red', 16), ('cause', 16), ('hate', 16), ('ok', 16), ('17', 16), ('mini', 16), ('favorite', 16), ('800', 16), ('tag', 16), ('n2', 16), ('investing', 16), ('monero', 16), ('nuse', 16), ('28', 16), ('xmr', 16), ('worked', 16), ('luck', 16), ('khlo', 16), ('argument', 16), ('kl6eqqtb67', 16), ('face', 15), ('videos', 15), ('cock', 15), ('porn', 15), ('nand', 15), ('candy', 15), ('able', 15), ('ahead', 15), ('energy', 15), ('country', 15), ('secrets', 15), ('network', 15), ('round', 15), ('limited', 15), ('information', 15), ('financial', 15), ('ive', 15), ('ways', 15), ('ann', 15), ('haven', 15), ('ya', 15), ('star', 15), ('merch', 15), ('product', 15), ('guide', 15), ('including', 15), ('house', 15), ('send', 15), ('months', 15), ('challenge', 15), ('quick', 15), ('nenter', 15), ('point', 15), ('class', 15), ('hes', 15), ('shoot', 15), ('90', 15), ('wont', 15), ('nto', 15), ('media', 15), ('announced', 15), ('birthday', 15), ('mean', 15), ('calling', 15), ('03', 15), ('airdrops', 15), ('xnk', 15), ('ltc', 15), ('startups', 15), ('eyes', 15), ('instead', 15), ('ago', 15), ('crying', 15), ('comment', 15), ('realdonaldtrump', 15), ('ft', 15), ('explosive', 15), ('gear', 15), ('dad', 15), ('bundle', 15), ('exactly', 15), ('spoilers', 15), ('150', 14), ('teen', 14), ('id', 14), ('apply', 14), ('payment', 14), ('cum', 14), ('hottest', 14), ('estate', 14), ('19', 14), ('test', 14), ('interesting', 14), ('school', 14), ('hosting', 14), ('enjoy', 14), ('ur', 14), ('sweet', 14), ('blonde', 14), ('debuts', 14), ('hoping', 14), ('wall', 14), ('manager', 14), ('local', 14), ('pm', 14), ('monday', 14), ('gave', 14), ('podcast', 14), ('jail', 14), ('opportunity', 14), ('given', 14), ('35', 14), ('series', 14), ('course', 14), ('experience', 14), ('till', 14), ('offers', 14), ('player', 14), ('trust', 14), ('mark', 14), ('pubg', 14), ('watching', 14), ('gotta', 14), ('collection', 14), ('recent', 14), ('omg', 14), ('gates', 14), ('icos', 14), ('looks', 14), ('mind', 14), ('playlist', 14), ('follower', 14), ('dna', 14), ('woman', 14), ('weight', 14), ('cosby', 14), ('secret', 13), ('deal', 13), ('small', 13), ('22', 13), ('premium', 13), ('sugar', 13), ('nfl', 13), ('fashion', 13), ('streaming', 13), ('interview', 13), ('saturday', 13), ('health', 13), ('sw', 13), ('prices', 13), ('age', 13), ('gorgeous', 13), ('upcoming', 13), ('releases', 13), ('major', 13), ('voting', 13), ('beautify', 13), ('space', 13), ('willing', 13), ('se', 13), ('dj', 13), ('tender', 13), ('grow', 13), ('education', 13), ('son', 13), ('saw', 13), ('writing', 13), ('build', 13), ('step', 13), ('block', 13), ('channel', 13), ('site', 13), ('attention', 13), ('visa', 13), ('self', 13), ('grab', 13), ('products', 13), ('single', 13), ('current', 13), ('industry', 13), ('kid', 13), ('400', 13), ('saying', 13), ('directly', 13), ('ecocoin', 13), ('cryptos', 13), ('newsoftheweek', 13), ('cofounder', 13), ('investments', 13), ('partner', 13), ('idrop', 13), ('mooreandgiles', 13), ('chances', 13), ('loss', 13), ('prom', 13), ('jewelry', 12), ('ceo', 12), ('invite', 12), ('tits', 12), ('horny', 12), ('asa', 12), ('akira', 12), ('kickstarter', 12), ('early', 12), ('works', 12), ('question', 12), ('dollar', 12), ('dream', 12), ('ads', 12), ('claim', 12), ('kitchen', 12), ('issue', 12), ('different', 12), ('ngive', 12), ('poor', 12), ('rich', 12), ('continue', 12), ('buying', 12), ('league', 12), ('daddy', 12), ('entertainment', 12), ('newsletter', 12), ('tool', 12), ('conference', 12), ('tweets', 12), ('forward', 12), ('hat', 12), ('events', 12), ('low', 12), ('outfit', 12), ('olympic', 12), ('canada', 12), ('anymore', 12), ('clean', 12), ('lead', 12), ('29', 12), ('answer', 12), ('cheap', 12), ('head', 12), ('investment', 12), ('turn', 12), ('past', 12), ('problem', 12), ('number', 12), ('lino', 12), ('autonomous', 12), ('mins', 12), ('universal', 12), ('currencies', 12), ('administration', 12), ('howtobuy', 12), ('demands', 12), ('breaking', 12), ('decided', 12), ('invested', 12), ('spankchain', 12), ('bigboobscoin', 12), ('easily', 12), ('weeks', 12), ('bc', 12), ('dabs', 12), ('piece', 12), ('sad', 12), ('cream', 12), ('entering', 12), ('friday', 12), ('mothersday', 12), ('nends', 12), ('beach', 12), ('booktour', 12), ('infinity', 12), ('lets', 12), ('anniversary', 12), ('taken', 12), ('draw', 12), ('blue', 11), ('sales', 11), ('pop', 11), ('nbusty', 11), ('lend', 11), ('60', 11), ('state', 11), ('annual', 11), ('crazy', 11), ('professional', 11), ('increase', 11), ('quality', 11), ('cross', 11), ('reminder', 11), ('nfor', 11), ('dress', 11), ('audio', 11), ('launches', 11), ('na', 11), ('simple', 11), ('office', 11), ('posts', 11), ('options', 11), ('signals', 11), ('african', 11), ('promotion', 11), ('data', 11), ('plan', 11), ('telegram', 11), ('stocks', 11), ('brunette', 11), ('lisa', 11), ('moment', 11), ('level', 11), ('lives', 11), ('ebook', 11), ('2017', 11), ('alert', 11), ('coupon', 11), ('powerful', 11), ('memoir', 11), ('dm', 11), ('artist', 11), ('history', 11), ('njoin', 11), ('losing', 11), ('proof', 11), ('scott', 11), ('mothers', 11), ('shot', 11), ('dmypys', 11), ('nbuy', 11), ('bethereum', 11), ('bthrbounty', 11), ('ex', 11), ('gone', 11), ('unveils', 11), ('nearly', 11), ('accounts', 11), ('ransom', 11), ('koreans', 11), ('developments', 11), ('definitely', 11), ('zero', 11), ('stupid', 11), ('inch', 11), ('cleavage', 11), ('mattress', 11), ('console', 11), ('supporting', 11), ('congrats', 11), ('bag', 11), ('die', 11), ('went', 11), ('johnlegend', 11), ('consensus', 11), ('sorry', 11), ('leg', 11), ('walked', 11), ('boycotts', 11), ('design', 10), ('deals', 10), ('wireless', 10), ('trouble', 10), ('phat', 10), ('date', 10), ('hello', 10), ('property', 10), ('samsung', 10), ('kill', 10), ('wearing', 10), ('expert', 10), ('al', 10), ('candidate', 10), ('lingerie', 10), ('deep', 10), ('sa', 10), ('ente', 10), ('solution', 10), ('literally', 10), ('parent', 10), ('demo', 10), ('themed', 10), ('started', 10), ('research', 10), ('users', 10), ('subscribe', 10), ('manyvids', 10), ('child', 10), ('wild', 10), ('fucked', 10), ('projects', 10), ('dare', 10), ('goals', 10), ('campaign', 10), ('fantastic', 10), ('sam', 10), ('smith', 10), ('track', 10), ('thriller', 10), ('donate', 10), ('celebrating', 10), ('understand', 10), ('add', 10), ('honor', 10), ('pl', 10), ('radio', 10), ('return', 10), ('dog', 10), ('backed', 10), ('line', 10), ('nmaking', 10), ('peace', 10), ('james', 10), ('1st', 10), ('short', 10), ('adoption', 10), ('universalbasicincome', 10), ('portfolio', 10), ('joined', 10), ('positive', 10), ('min', 10), ('volume', 10), ('dash', 10), ('bch', 10), ('agree', 10), ('popped', 10), ('cyberattacks', 10), ('16', 10), ('cryptonews', 10), ('security', 10), ('5th', 10), ('closer', 10), ('passes', 10), ('push', 10), ('com', 10), ('welcome', 10), ('glad', 10), ('selfie', 10), ('ending', 10), ('regret', 10), ('4k', 10), ('irepairfast', 10), ('enthoo', 10), ('evolv', 10), ('itx', 10), ('bookboost', 10), ('aw', 10), ('bts_twt', 10), ('matter', 10), ('water', 10), ('em', 10), ('told', 10), ('wouldnt', 10), ('koscielny', 10), ('report', 9), ('sport', 9), ('feb', 9), ('coral', 9), ('meant', 9), ('eye', 9), ('kendall', 9), ('asian', 9), ('picture', 9), ('yummy', 9), ('medal', 9), ('discord', 9), ('yey', 9), ('see_snaps', 9), ('toyota', 9), ('sharing', 9), ('jay', 9), ('inlinkz', 9), ('linkup', 9), ('cu', 9), ('fo', 9), ('discover', 9), ('broken', 9), ('room', 9), ('plays', 9), ('doubt', 9), ('gun', 9), ('mvsales', 9), ('abuse', 9), ('paying', 9), ('highlights', 9), ('double', 9), ('vivo', 9), ('uses', 9), ('shipping', 9), ('united', 9), ('hosted', 9), ('le', 9), ('scene', 9), ('key', 9), ('access', 9), ('95', 9), ('press', 9), ('legend', 9), ('swiss', 9), ('highly', 9), ('storm', 9), ('thomas', 9), ('sun', 9), ('mystery', 9), ('youth', 9), ('toy', 9), ('simply', 9), ('john', 9), ('version', 9), ('planning', 9), ('pressure', 9), ('65', 9), ('celebrates', 9), ('higher', 9), ('creating', 9), ('seriously', 9), ('spend', 9), ('exclusive', 9), ('cnbc', 9), ('startup', 9), ('tokensale', 9), ('light', 9), ('everyday', 9), ('replace', 9), ('philanthropy', 9), ('plans', 9), ('exchanges', 9), ('dallas', 9), ('knows', 9), ('kind', 9), ('regulation', 9), ('yesterday', 9), ('5000', 9), ('headlines', 9), ('shade', 9), ('kuwtk', 9), ('jets', 9), ('gi', 9), ('gamersoutreach', 9), ('nzxt', 9), ('concert', 9), ('announcement', 9), ('digger', 9), ('fandango', 9), ('nas', 9), ('starbucks', 9), ('writes', 9), ('attire', 9), ('okay', 9), ('idk', 9), ('damn', 9), ('lmao', 9), ('atleti', 9), ('bro', 9), ('defending', 9)]\n",
      "1253\n"
     ]
    }
   ],
   "source": [
    "isSpam = df['label'] == '1'\n",
    "notSpam = df['label'] == '0'\n",
    "spam = df[isSpam]\n",
    "spam = spam['tweet']\n",
    "spamwords = count_vector.fit_transform(spam)\n",
    "sum_words = spamwords.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vector.vocabulary_.items()]\n",
    "print(sorted(words_freq, key = lambda x: x[1], reverse=True))\n",
    "print(len(sorted(words_freq, key = lambda x: x[1], reverse=True)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sorts every word in tweets not labeled spam by frequency. The most frequent words are kardashian(1045 appearances), kim(522), cryptocurrency(242), new(231) and like(204). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('https', 4510), ('kardashian', 1045), ('kim', 522), ('rt', 420), ('cryptocurrency', 242), ('new', 231), ('like', 204), ('just', 200), ('click', 182), ('nhttps', 176), ('shopping', 156), ('java', 154), ('khloe', 151), ('people', 149), ('kardashians', 143), ('data', 127), ('playing', 125), ('baby', 124), ('ai', 118), ('jenner', 112), ('love', 105), ('trump', 99), ('good', 98), ('amp', 97), ('make', 93), ('im', 92), ('bigdata', 91), ('kourtney', 90), ('time', 88), ('listen', 83), ('west', 81), ('president', 81), ('bitcoin', 81), ('know', 77), ('free', 77), ('look', 76), ('great', 76), ('machinelearning', 75), ('datascience', 75), ('news', 73), ('8rnluayhzd', 72), ('day', 71), ('come', 70), ('think', 69), ('north', 69), ('abdsc', 69), ('dont', 68), ('congress', 68), ('need', 67), ('house', 67), ('learning', 67), ('video', 65), ('today', 63), ('going', 63), ('year', 61), ('really', 61), ('watch', 61), ('kylie', 61), ('big', 59), ('facebook', 59), ('japan', 59), ('pink', 58), ('2018', 58), ('years', 58), ('korea', 56), ('youtube', 55), ('blockchain', 54), ('shows', 53), ('want', 52), ('say', 52), ('mexicos', 52), ('machine', 52), ('kids', 51), ('approves', 51), ('says', 51), ('best', 50), ('artificialintelligence', 50), ('10', 49), ('app', 49), ('join', 49)]\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "ham = df[notSpam]\n",
    "ham = ham['tweet']\n",
    "hamwords = count_vector.fit_transform(ham)\n",
    "\n",
    "sum_words = hamwords.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in count_vector.vocabulary_.items()]\n",
    "print(sorted(words_freq, key = lambda x: x[1], reverse=True))\n",
    "print(len(sorted(words_freq, key = lambda x: x[1], reverse=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharimkhan/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tweet'], df['label'], random_state=1)\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the total set: 13058\n",
      "Number of rows in the training set: 9793\n",
      "Number of rows in the test set: 3265\n"
     ]
    }
   ],
   "source": [
    "print('Number of rows in the total set: {}'.format(df.shape[0]))\n",
    "print('Number of rows in the training set: {}'.format(X_train.shape[0]))\n",
    "print('Number of rows in the test set: {}'.format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Accuracy score:  0.8539050535987749\n",
      "Precision score:  0.8539050535987749\n",
      "Recall score:  0.8539050535987749\n",
      "F1 score:  0.8539050535987749\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "print(\"SVM\")\n",
    "clf = svm.SVC(probability=True, C=1000)\n",
    "clf.fit(training_data,y_train)\n",
    "svmpredictions = clf.predict(testing_data)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy score: ', format(accuracy_score(y_test,svmpredictions )))\n",
    "print('Precision score: ', format(precision_score(y_test,svmpredictions,average='micro' )))\n",
    "print('Recall score: ', format(recall_score(y_test,svmpredictions,average='micro' )))\n",
    "print('F1 score: ', format(f1_score(y_test,svmpredictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.60822418\n",
      "Iteration 2, loss = 0.28317016\n",
      "Iteration 3, loss = 0.21682388\n",
      "Iteration 4, loss = 0.17481220\n",
      "Iteration 5, loss = 0.14128008\n",
      "Iteration 6, loss = 0.11182816\n",
      "Iteration 7, loss = 0.09270734\n",
      "Iteration 8, loss = 0.07989894\n",
      "Iteration 9, loss = 0.07238263\n",
      "Iteration 10, loss = 0.06618951\n",
      "Iteration 11, loss = 0.06252192\n",
      "Iteration 12, loss = 0.06240368\n",
      "Iteration 13, loss = 0.05746110\n",
      "Iteration 14, loss = 0.05629867\n",
      "Iteration 15, loss = 0.05393966\n",
      "Iteration 16, loss = 0.05351072\n",
      "Iteration 17, loss = 0.05041549\n",
      "Iteration 18, loss = 0.05032314\n",
      "Iteration 19, loss = 0.04915702\n",
      "Iteration 20, loss = 0.04875020\n",
      "Iteration 21, loss = 0.04769428\n",
      "Iteration 22, loss = 0.04738353\n",
      "Iteration 23, loss = 0.04846534\n",
      "Iteration 24, loss = 0.04737059\n",
      "Iteration 25, loss = 0.04492462\n",
      "Iteration 26, loss = 0.04526192\n",
      "Iteration 27, loss = 0.04461115\n",
      "Iteration 28, loss = 0.04484084\n",
      "Iteration 29, loss = 0.04412003\n",
      "Iteration 30, loss = 0.04404542\n",
      "Iteration 31, loss = 0.04427466\n",
      "Iteration 32, loss = 0.04370005\n",
      "Iteration 33, loss = 0.04348140\n",
      "Iteration 34, loss = 0.04372960\n",
      "Iteration 35, loss = 0.04436130\n",
      "Iteration 36, loss = 0.04245310\n",
      "Iteration 37, loss = 0.04352024\n",
      "Iteration 38, loss = 0.04339703\n",
      "Iteration 39, loss = 0.04267155\n",
      "Training loss did not improve more than tol=0.000000 for two consecutive epochs. Stopping.\n",
      "Accuracy score:  0.8609494640122511\n",
      "Precision score:  0.8609494640122511\n",
      "Recall score:  0.8609494640122511\n",
      "F1 score:  0.8609494640122511\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,100,100), max_iter=500, alpha=0.0001,\n",
    "                     solver='adam', verbose=10,  random_state=21,tol=0.000000001)\n",
    "clf.fit(training_data,y_train)\n",
    "predictions = clf.predict(testing_data)\n",
    "print(\"Multi-Layer Perceptron\")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy score: ', format(accuracy_score(y_test,predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test,predictions,average='micro')))\n",
    "print('Recall score: ', format(recall_score(y_test,predictions,average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_test,predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "Accuracy score:  0.845635528330781\n",
      "Precision score:  0.845635528330781\n",
      "Recall score:  0.845635528330781\n",
      "F1 score:  0.845635528330781\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data,y_train)\n",
    "predictions = naive_bayes.predict(testing_data)\n",
    "print(\"Naive Bayes\")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy score: ', format(accuracy_score(y_test,predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test,predictions,average='micro')))\n",
    "print('Recall score: ', format(recall_score(y_test,predictions,average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_test,predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "Accuracy score:  0.8704441041347626\n",
      "Precision score:  0.8704441041347626\n",
      "Recall score:  0.8704441041347626\n",
      "F1 score:  0.8704441041347626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharimkhan/anaconda/lib/python3.6/site-packages/sklearn/linear_model/sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression\")\n",
    "from sklearn import linear_model\n",
    "regr = linear_model.LogisticRegression(random_state=0, solver='sag')\n",
    "regr.fit(training_data,y_train)\n",
    "predictions = regr.predict(testing_data)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test,predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test,predictions,average='micro')))\n",
    "print('Recall score: ', format(recall_score(y_test,predictions,average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_test,predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "Accuracy score:  0.8698315467075038\n",
      "Precision score:  0.8698315467075038\n",
      "Recall score:  0.8698315467075038\n",
      "F1 score:  0.8698315467075038\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "print(\"Random Forest\")\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=0, criterion='gini')\n",
    "clf.fit(training_data,y_train)\n",
    "predictions = clf.predict(testing_data)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test,predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test,predictions,average='micro')))\n",
    "print('Recall score: ', format(recall_score(y_test,predictions,average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_test,predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree\n",
      "Accuracy score:  0.8563552833078101\n",
      "Precision score:  0.8563552833078101\n",
      "Recall score:  0.8563552833078101\n",
      "F1 score:  0.8563552833078102\n"
     ]
    }
   ],
   "source": [
    "print(\"Tree\")\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier(criterion='gini', random_state=0)\n",
    "clf.fit(training_data,y_train)\n",
    "predictions = clf.predict(testing_data)\n",
    "print('Accuracy score: ', format(accuracy_score(y_test,predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test,predictions,average='micro')))\n",
    "print('Recall score: ', format(recall_score(y_test,predictions,average='micro')))\n",
    "print('F1 score: ', format(f1_score(y_test,predictions,average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
